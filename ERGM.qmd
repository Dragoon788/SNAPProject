---
title: "Final Proj"
author: "Francis Velasco, Tosin Okoh, Evan Bertis-Sample"
date: today 
format: 
  pdf:
      toc: true
      toc-depth: 4
      shift-heading-level-by: 2
      fig-pos: "H"
      fig-cap-location: top
      geometry:
        - top=1in
        - right=.8in
        - bottom=1in
        - left=.8in
      link-citations: true
      linkcolor: blue
      include-in-header: 
        text: |
          \usepackage{fancyhdr}
          \usepackage{titling}
          \pagestyle{fancy}
          \fancyhf{}
          \renewcommand\maketitle{
            \fancyhead[C]{
              \thetitle
              \ifx \theauthor\empty  \else \ – \theauthor \fi
              \ifx \thedate\empty  \else \ – \thedate \ \fi
            }
          }
          \fancyfoot[C]{\thepage}
---

## File Initialization

```{r}
#| echo: false
#| output: false
#| message: false
#| label: load packages

# Start with a clear environment
rm(list=ls())

# Loading packages 
library(dplyr)
library(igraph)
library(ggplot2)
library(poweRlaw)
library(readr)
library(tidytext)
library(tidygraph)
library(ggraph)
library(tidyverse)
library(topicmodels)
library(textstem)
library(udpipe)

```

```{r}
#| echo: false
#| output: false
#| message: false
#| label: load data

## Loading Edgelist Data for our Twitch Social Network
## Edges represent viewership overlap between Twitch streamers
setwd("C:/Users/coolb/OneDrive/Documents/SNA Class/FinalProj")

edgelist <- read.csv("Updated&Cleaned-Edgelist.csv")

# generate a labeled graph from edgelist
# data_graph represents the Twitch Social Network
data_graph <- graph_from_data_frame(edgelist) |>
  as_tbl_graph() |>
  as.undirected()
```

## Calculating Centrality

```{r}
#| echo: false
#| output: false
#| message: false
#| label: giant component

# Isolating the largest component of the graph to do analysis
data_comp <- components(data_graph)
giantGraph_data <- data_graph %>%
  induced_subgraph(., which(data_comp$membership == which.max(data_comp$csize)))
vcount(data_graph) ## total # number of nodes/twitch channels
ecount(data_graph) ## total # number of edges/shared viewers

```

### Centrality Metrics Table of Top 10 Nodes in each metric

```{r}
#| label: centrality metrics

#### TAKEN FROM LAB 2

# For this part, you switch 'igraph' to 'sna' package because we are going to use 
# some functions that only are available in sna package
# As a first step, create a 'sna' graph object from an 'igraph' object
sna_data <-
  igraph::as_adjacency_matrix(giantGraph_data, sparse = FALSE) %>% network::as.network.matrix()

# this detaching is a necessary step since the two packages have some same function names
# R is often confused
detach('package:igraph')
library(statnet)

# We will compute centralities based on 'network' package
# First, create a dataframe to store the centrality information
centralities_data <- data.frame('node_name' = as.character(network.vertex.names(sna_data)))

####

# Calculating the centrality metrics for all the nodes/Streamers in our network
## Presented in a table arranged by the betweeness centrality

centralities_data$betweenness <- igraph::betweenness(
  giantGraph_data,
  directed = FALSE, # Adjust this based on whether your graph is directed or not
  normalized = TRUE # Adjust if you want normalized betweenness scores
  )
centralities_data$degree <- degree(sna_data, cmode = 'freeman')/2
centralities_data$closeness <-
  igraph::closeness(
    giantGraph_data, 
    mode = 'all'
    )
centralities_data$eigen <-
  igraph::eigen_centrality(giantGraph_data)$vector

# Select top 10 nodes by betweenness
centralities_data |> 
  dplyr::slice_max(order_by = betweenness, n = 10)

#Discussion_Betweeness <- betweenness(giantGraph_data)
#Discussion_Betweeness <- as.data.frame(Discussion_Betweeness)

#set.seed (3952)
#layout1 <- layout.fruchterman.reingold(giantGraph_data, niter = 500)
#V(giantGraph_data)$size=betweenness(giantGraph_data)/200
#E(giantGraph_data)$color <- "grey"

#plot(giantGraph_data)
```

### Plotting Betweenness Centralities

```{r}
#reload igraph library again because we detached it earlier
library(igraph)

# Compute betweenness centrality
betweenness_centrality <- centralities_data$betweenness

# Identify top 10 nodes by betweenness centrality
top_nodes <- order(betweenness_centrality, decreasing = TRUE)[1:10]

# Create a vector for labels, with NA for nodes not in the top 10
vertex_labels <- rep(NA, vcount(giantGraph_data))  # Use vcount(giantGraph_data) to get the number of vertices
vertex_labels[top_nodes] <- V(giantGraph_data)$name[top_nodes]  # Assign labels to the top 10 nodes

# Plot the graph
plot(giantGraph_data, 
     vertex.size = betweenness_centrality * 10,  # Adjust node size
     vertex.label = vertex_labels,  # Display labels only for top 10 nodes
     main = "Node Betweenness Centrality",
     layout = layout.fruchterman.reingold)  # Use Fruchterman-Reingold layout

```

### Plotting Bridges:

```{r}
# Find bridges in the graph
bridges_edges <- bridges(data_graph)

# Create a data frame to store bridge information
bridges_df <- data.frame(
  edge = as.character(bridges_edges),
  from = sapply(bridges_edges, function(e) ends(data_graph, e)[1]),
  to = sapply(bridges_edges, function(e) ends(data_graph, e)[2])
)

# Function to calculate the number of components created by removing an edge
bridge_scores <- sapply(bridges_edges, function(e) {
  subgraph <- delete_edges(data_graph, e)
  length(components(subgraph)$csize)
})

# Add the scores to the data frame
bridges_df$score <- bridge_scores

# Rank the bridges based on their scores
bridges_df <- bridges_df[order(-bridges_df$score), ]

# Display the ranked bridges
print(bridges_df)

# Create a vector to define edge colors and widths
edge_colors <- rep("gray", ecount(data_graph))  # Default color for non-bridges
edge_widths <- rep(1, ecount(data_graph))  # Default width for non-bridges

# Highlight bridges
edge_colors[E(data_graph) %in% bridges_edges] <- "red"
edge_widths[E(data_graph) %in% bridges_edges] <- 20  # Thicker lines for bridges

# Plot the graph
plot(data_graph,
     edge.color = edge_colors,
     edge.width = edge_widths,
     vertex.size = 5,
     vertex.label = NA,
     main = "Nodes that Act as Bridges")

```

## Louvain Community Detection Method

```{r}
#| label: community detection

library(igraph)

## Calculate cluster information for both betweenness and louvain method
cluster <- giantGraph_data %>% cluster_louvain()

## Group clusters together to make it easier to see
COMP = components(data_graph)
LC = which(COMP$membership == 1)
LargeComp = induced_subgraph(data_graph, LC)

LC.gn.comm <- cluster_louvain(LargeComp)
LC_Grouped = LargeComp
E(LC_Grouped)$weight = 1
for(i in unique(membership(LC.gn.comm))) {
    GroupV = which(membership(LC.gn.comm) == i)
    LC_Grouped = add_edges(LC_Grouped, combn(GroupV, 2), attr=list(weight=6))
} 
```

dplyr::summarize(

num_nodes = n(),

)

```{r}
#| label: community detection 2

length(cluster) # number of clusters

# Find the size of each cluster
sizes(cluster)  

# Get the membership vector from the cluster object
membership_vector <- membership(cluster)

# Create a data frame with node information and community membership
node_info <- data.frame(
  node = V(giantGraph_data)$name,
  community = membership_vector
)

# Group the nodes by their community
nodes_by_community <- node_info %>%
  dplyr::group_by(community) %>%
  dplyr::summarize(nodes = paste(node, collapse = ", "))

# Print the nodes in each community
print(nodes_by_community)
```

## Calculating Modularity Score

```{r}
#| label: modularity


# Calculating modularity score
print("Calculated Modularity Score for Twitch Viewership Overlap:")
modularity(cluster)
```

Interpret the modularity score of your results of community detection.

A modularity score quantifies the strength of division of a network into modules. It is the fraction of edges that fall in a given cluster minus the expected fraction if edges were distributed at random. Modularity scores range form (-1, 1)

My modularity scores for both the Viewership Overlap network is 0.50. This is a fairly high value as described before, meaning there are dense connections between nodes in a cluster and sparse connections between random nodes. This also means that there are fairly distinct communities found in our data.

## Plot the communities! **(6 points)**

```{r}
#| label: community detection 3


#### TAKEN FROM LAB 2

# Visualize clusters - that puts colored blobs around the nodes in the same community.
# You may want to remove vertex.label=NA to figure out what terms are clustered.
cluster %>% plot(
  .,
  giantGraph_data,
  # layout = layout_with_gem(.),
  layout = layout_with_fr(giantGraph_data),
  edge.arrow.size = .3,
  vertex.size = 4,
  vertex.label = NA,
  vertex.color = adjustcolor(membership(.), alpha.f = .3),
  vertex.label.cex = .5,
  vertex.label.color = 'black',
  mark.groups = by(seq_along(membership(.)), membership(.), invisible),
  mark.shape = 1 / 4,
  mark.col = rainbow(length(.), alpha = .1),
  mark.border = NA
)

#####

GN.Comm = simplify(contract(LargeComp, membership(LC.gn.comm)))
D = unname(degree(GN.Comm))

set.seed(1234)
par(mar=c(0,0,0,0))
plot(GN.Comm, vertex.size=sqrt(sizes(LC.gn.comm)),
    vertex.label=1:43, vertex.cex = 0.8,
    vertex.color=round(log(D))+1)

set.seed(1234)
LO = layout_with_fr(LC_Grouped)
colors <- rainbow(max(membership(LC.gn.comm)))
par(mar=c(0,0,0,0))
plot(LC.gn.comm, LargeComp, layout=LO,
    vertex.size = 6, 
    vertex.color=colors[membership(LC.gn.comm)], 
    vertex.label = NA, edge.width = 1)

```

```{r}
#| label: degree distribution
# Examine the in-degree distribution
data.frame(density = giantGraph_data %>% 
             degree_distribution(mode = "in")) |> 
  dplyr::mutate(degree = row_number()) |> 
  ggplot(aes(x = degree, y = density)) +
  geom_bar(stat = "identity") +
  ggtitle("In-degree Distribution for our Streamer Data") +
  ylab("Density") +
  xlab("In-degree")+
  theme_minimal()

# CCDF - Complementary Cumulative Distribution Function
# Plot a log-log plot of in-degree distribution
data.frame(density = giantGraph_data %>% 
             degree_distribution(mode = "in", cumulative = TRUE)) |> 
  dplyr::mutate(degree = row_number()) |>
  ggplot(aes(x = degree, y = density)) +
  geom_line() +
  scale_x_log10() +
  scale_y_log10() +
  ggtitle("Log-Log Plot of In-degree for our Streamer Data") +
  ylab("CCDF") +
  xlab("In-degree")+
  theme_minimal()


# Fit a power law to the degree distribution
# The output of the power.law.fit() function tells us what the exponent of the power law is ($alpha)
# and the log-likelihood of the parameters used to fit the power law distribution ($logLik)
# Also, it performs a Kolmogov-Smirnov test to test whether the given degree distribution could have
# been drawn from the fitted power law distribution.
# The function thus gives us the test statistic ($KS.stat) and p-vaule ($KS.p) for that test

set.seed(1)
my_power_law_dist = poweRlaw::rpldis(200, xmin = 1, alpha = 3)-1  ## lest create a random power law distribution

# my_power_law_dist
in_power_data <- giantGraph_data %>%
  degree.distribution(., mode = 'in')  %>%
  fit_power_law(.)
in_power_data

# finally we will do a Kolmogov-Smirnov test against power law distribution that we created earlier
# here you will see a p-value
giantGraph_data %>%
  degree.distribution(., mode = 'in') |> 
  ks.test(my_power_law_dist)
```

------------------------------------------------------------------------

```{r}
#| echo: false
#| output: false
#| message: false

rm(list=ls())

# Lab 3:
# Exponential Random Graph Models (ERGMs)

# Clear your environment
rm(list=ls())

# Install packages below if you do not have them:
# -------------------------------------------------
if (!"statnet" %in% installed.packages()) install.packages("statnet") # For fitting ERGMs
if (!"igraph" %in% installed.packages()) install.packages("igraph") # For network plotting
if (!"texreg" %in% installed.packages()) install.packages("texreg") # For printing "nicer" model output

library(statnet)

# -------------------------------------------------------------------------------------------------
# Set the working directory
# Session > Set Working Directory > To Source File Location
# -------------------------------------------------------------------------------------------------
list.files() # List the files in the current working directory to see if you're in the right directory

```

```{r}
#| echo: false
#| output: false
#| message: false

######################## PART I: Building and Visualizing the Networks ########################
# ----------------------------------------------------------------------------------------------------
# Dependent variable:
# Responses to the question:
#weight is the overlapping viewers!
ov_viewerEdgelist <- read.csv("Updated&Cleaned-Edgelist.csv")
# View the first rows of the edgelist to make sure it imported correctly:
head(ov_viewerEdgelist)

# Convert the edgelist to a network object in statnet format:
overlap_view <- as.network.matrix(ov_viewerEdgelist, matrix.type = "edgelist")
# View a summary of the network object
overlap_view

# Independent variables:
# Load node attributes, and store them in the advice network object we have created
overlap_view |> network::set.vertex.attribute("Channel_Name",
                          value = read.csv("new_updated_channels_with_count.csv")$Channel_Name) # Categorical variable for channel name. I dont think this one is all that neccesary since they are all unique values but you never know so I added it in lol

overlap_view |> network::set.vertex.attribute("Language",
                          value = read.csv("new_updated_channels_with_count.csv")$Language) # Categorical variable for lang

overlap_view |> network::set.vertex.attribute("Tags",
                          value = read.csv("new_updated_channels_with_count.csv")$Tags) # Categorical variable for tags

overlap_view |> network::set.vertex.attribute("Last_Played_Game",
                          value = read.csv("new_updated_channels_with_count.csv")$Last_Played_Game)
# Last game played

overlap_view |> network::set.vertex.attribute("Game_ID",
                          value = read.csv("new_updated_channels_with_count.csv")$`Game_ID`) # Categorical variable for Game ID, The ID of the game that the user plays. The game is not updated if the ID isn’t a game ID that Twitch recognizes.

overlap_view |> network::set.vertex.attribute("ChannelID",
                          value = read.csv("new_updated_channels_with_count.csv")$ChannelID)
#Channel_ID = Unique ID given to each user

overlap_view |> network::set.vertex.attribute("Count",
                          value = read.csv("new_updated_channels_with_count.csv")$Count)
# Count = number of unique viewers

overlap_view |> network::set.vertex.attribute("followers",
                          value = read.csv("new_updated_channels_with_count.csv")$followers) #with followers


overlap_view # These 8 variables should now be listed as vertex attributes when viewing the summary of the network
```

```{r}
# # Double-check the values for all of the node variables
#### Note: looks good to me! It's just a lot so I commented it out!

# network::get.vertex.attribute(overlap_view,"Channel_Name")
# network::get.vertex.attribute(overlap_view,"Tags")
# network::get.vertex.attribute(overlap_view,"Language")
# network::get.vertex.attribute(overlap_view,"Last_Played_Game")
# network::get.vertex.attribute(overlap_view,"Game_ID")
# network::get.vertex.attribute(overlap_view,"ChannelID")
# network::get.vertex.attribute(overlap_view,"Count")
# network::get.vertex.attribute(overlap_view,"followers")
```

```{r}
# ----------------------------------------------------------------------------
######################## PART II: Build the ERGM models ########################
#
# R vignette for more details: https://cran.r-project.org/web/packages/ergm/ergm.pdf
# ----------------------------------------------------------------------------
#detach(package:igraph) # Remove the 'igraph' package from your environment. 
library(statnet)
options(ergm.loglik.warn_dyads=FALSE) #Whether or not a warning should be issued when sample space constraints render the observed number of dyads ill-defined

# Ergm Terms are statistics: They are some deterministic function of the ties, node attributes, and edge covariates of a network.
help("ergm-terms",package = "ergm") # Documentation that contains definitions for all of the terms we are using
                                    # ex. what does "mutual" test and how is it calculated
# We will use the ergm-terms to perform hypothesis testing using ERGMs
# But we can note that any of the ERGM terms can also be examined directly for your observed network, by creating a formula in R

# Look at Endogenous statistics: terms based on only ties in the advice network
summary(overlap_view ~ edges)                     # Number of edges (ties)
#dont really need mutual, the ties are not too likely to be mutual
summary(overlap_view ~ odegree(0:10))              # Outdegree distribution. (# of nodes with outdegree of 0, # nodes outdegree of 1, etc., playing with this number, will increase to 10 from 5 (7:19pm 8/21))
                                           
summary(overlap_view ~ idegree(0:100))             # Indegree distribution. (increased to 100 from 65, will decide later whether to increase it more!)
summary(overlap_view ~ gwodegree(log(2),fixed=T)) # One parameter summarizing outdegree distribution - tendency against outdegree hubs
summary(overlap_view ~ gwidegree(log(2),fixed=T)) # One parameters summarizing indegree distribution - tendency against indegree hubs
summary(overlap_view ~ desp(1:10))                 # Pairs of nodes with one shared partner, two shared partners, etc. (increasing up from 5 to 10 shared partners!)
summary(overlap_view ~ dgwesp(log(2),fixed = T))  # One parameter summarizing 

# Look at Exogenous statistics: terms based on advice ties AND other ties / node attributes

summary(overlap_view ~ nodeocov("Count"))             #examines whether nodes with different values of the Count attribute (number of unique viewers) have varying probabilities of being connected to other nodes. Helps determine if nodes with higher or lower viewer counts are more or less likely to have edges between them.

summary(overlap_view ~ nodematch("Language"))            # Number of ties between users of same language
#Note: Note sure if as important but will see

summary(overlap_view ~ nodematch("Tags"))        # Number of ties between users of the same tag

summary(overlap_view ~ nodematch("Last_Played_Game"))        # Number of ties between people who have the same last game played

summary(overlap_view ~ nodeicov("followers"))        #assess whether users with higher follower counts are more likely to receive connections from others.

summary(overlap_view ~ nodeocov("followers"))      
```

#simple model; a complex model attempt was made but no progress

```{r}
#adding in things like gwidegree, gwodegree, gwodegree, dgwesp is too complex for the model right now
model_simple <- ergm(overlap_view ~ edges 
                     + nodeocov("followers")
                     + nodeicov("followers")
                     + nodeocov("Count") # Covariance between out-degree of nodes and attributes of nodes
                     + nodematch("Language") # Homophily on lang
                     + nodematch("Tags")  #Homophily on tags
                     + nodematch("Last_Played_Game") #Homophily on last played games
               # Control settings for MCMC-MLE algorithm
               , constraints =~ bd(maxout= 100000000) #it wont be an mcmc without me doing this weirdly high value
               , control = control.ergm(MCMC.effectiveSize = 50,
                                        seed = 42)
)
summary(model_simple)
```

#model diagnostics for simple model

```{r}
par(mar=c(2,2,2,2))
mcmc.diagnostics(model_simple)
```

#goodness of fit

```{r}
#| warning: false
# Simple Model:
# It may take a second for this command to run.
gof1 <- gof(model_simple, verbose=T, burnin=1e+5, interval=1e+5, control = control.gof.ergm(nsim = 50))
# If you run below and then wouldn't see the plot, try par(mar=c(2,2,2,2))
#dev.off()
plot(gof1)
gof1
```
